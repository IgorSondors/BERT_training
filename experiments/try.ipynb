{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "    (position_embeddings): Embedding(2048, 312)\n",
       "    (token_type_embeddings): Embedding(2, 312)\n",
       "    (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "          (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "          (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "          (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings#[0].cpu().numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3304e-02, -1.6766e-02,  7.0637e-03, -6.2864e-02,  1.5361e-02,\n",
       "          2.5157e-02, -2.9619e-02, -6.4186e-02, -8.6948e-02, -1.5729e-02,\n",
       "          4.8932e-02,  7.9619e-02,  7.1070e-02,  1.9035e-02,  2.1821e-02,\n",
       "         -7.7134e-02,  6.4271e-02, -1.8979e-02, -2.6368e-02,  2.5588e-02,\n",
       "         -6.4028e-03,  3.7539e-02,  5.6162e-02, -7.6889e-04,  8.8052e-02,\n",
       "         -9.0258e-03, -4.4325e-02,  5.5979e-03, -2.9022e-02,  6.0026e-02,\n",
       "         -3.9192e-02, -5.9564e-02,  1.0481e-01, -2.7476e-02, -2.0924e-02,\n",
       "          6.1448e-02, -3.7913e-02, -3.5064e-02, -1.1319e-01, -7.2562e-03,\n",
       "         -1.2947e-01,  1.3300e-01,  1.1113e-01, -1.0539e-01,  1.8497e-02,\n",
       "          6.7829e-02,  1.7365e-02, -1.5973e-02,  1.0037e-03, -6.8340e-02,\n",
       "          6.0094e-02, -6.0754e-02,  1.1632e-02, -1.8764e-02, -7.2381e-02,\n",
       "          7.4951e-02,  1.1092e-01, -1.3611e-02, -2.5881e-02, -2.4048e-02,\n",
       "          1.7180e-02,  1.0572e-01, -3.5280e-02,  6.9378e-02,  2.7238e-02,\n",
       "          7.1891e-02,  4.9430e-02,  3.9346e-02, -7.3765e-02,  5.1603e-02,\n",
       "          6.7091e-02, -6.2704e-02, -9.9453e-02, -4.9381e-02,  3.4577e-02,\n",
       "         -1.6885e-02, -2.9075e-02, -3.0091e-03, -6.3649e-02,  5.8020e-02,\n",
       "          2.8718e-02,  1.0310e-01, -1.5348e-03,  2.4713e-03, -1.2471e-02,\n",
       "          1.8491e-02, -4.7750e-03, -1.0194e-02,  6.9409e-02,  1.1711e-02,\n",
       "          1.6948e-02,  1.9900e-02,  1.0202e-01, -2.0864e-02,  6.7053e-03,\n",
       "          4.3094e-02,  6.7626e-02,  9.4867e-02, -7.9791e-02, -5.3607e-03,\n",
       "         -2.4589e-02, -1.5848e-02, -3.5178e-02, -7.6930e-02, -2.2956e-02,\n",
       "         -6.1423e-02, -6.0613e-03, -2.8450e-03,  1.6953e-02, -4.3384e-02,\n",
       "          7.0469e-02,  3.9711e-02,  9.0310e-02,  1.1443e-01,  4.7188e-02,\n",
       "          8.3116e-02, -8.1253e-02, -7.5366e-03,  1.4504e-02, -5.1950e-02,\n",
       "          2.9388e-02, -4.1701e-02, -5.3027e-02,  5.7552e-02, -2.1664e-02,\n",
       "         -6.4374e-03,  2.9999e-02,  7.5984e-02,  4.6394e-02,  4.4674e-02,\n",
       "         -9.4018e-02,  2.1046e-02, -7.4057e-03,  3.3324e-02,  1.9890e-02,\n",
       "          3.0821e-02, -2.1268e-02,  5.1843e-02,  3.8360e-02, -6.2449e-03,\n",
       "          2.3698e-02,  7.0568e-02,  6.8309e-02, -1.5426e-02,  3.2048e-02,\n",
       "         -6.8709e-02,  3.6938e-02,  5.7439e-03,  4.6279e-02,  3.8387e-02,\n",
       "          3.2671e-02,  5.0414e-02,  7.0698e-03, -4.9726e-02, -1.8381e-02,\n",
       "          3.9911e-03,  2.5733e-02,  2.0368e-02, -6.3489e-02, -3.4124e-02,\n",
       "         -2.4336e-03,  6.7478e-02, -9.0538e-03, -5.8829e-02, -2.5972e-02,\n",
       "         -5.0324e-02, -2.3875e-03, -1.9870e-03, -1.9571e-02,  5.8233e-03,\n",
       "          1.6014e-01,  1.1468e-01,  1.8356e-03,  7.8796e-02, -4.9952e-02,\n",
       "         -9.7345e-03,  1.0437e-02, -1.5769e-02, -2.9454e-02,  8.2080e-02,\n",
       "          4.4876e-02, -3.1302e-02,  4.8019e-02,  1.1416e-02,  1.1087e-02,\n",
       "          3.5016e-02,  2.6509e-02,  3.8078e-02, -7.7058e-03,  3.0528e-02,\n",
       "         -2.2514e-02, -1.1013e-02, -2.0789e-02,  1.3491e-02,  4.5603e-02,\n",
       "         -3.8195e-01,  4.0627e-02,  1.0461e-01,  3.1417e-03,  1.7778e-02,\n",
       "          9.8166e-02, -7.7672e-02, -3.2572e-02, -3.3885e-02,  1.0198e-01,\n",
       "         -2.7450e-02,  7.1298e-02, -9.7953e-03, -1.6457e-02,  5.7943e-03,\n",
       "         -9.9575e-02, -4.6282e-02, -4.7879e-02,  2.6492e-03,  1.8991e-02,\n",
       "         -1.2092e-01,  4.7299e-02, -3.4702e-02, -6.4862e-03, -5.9568e-02,\n",
       "         -8.1953e-03,  5.4928e-02, -2.2361e-02,  8.0468e-02,  6.7801e-02,\n",
       "         -4.5734e-02, -6.6899e-04,  6.2527e-02, -4.8074e-02,  7.8176e-03,\n",
       "         -1.0752e-02, -4.6293e-02,  2.9420e-02, -5.9063e-02,  5.2140e-02,\n",
       "          3.4703e-02,  2.1490e-02, -1.0563e-01, -2.8468e-02, -6.5586e-03,\n",
       "         -2.4203e-02, -8.9006e-02, -1.7710e-04, -5.7601e-02, -1.5660e-02,\n",
       "          1.8924e-02, -6.7216e-03, -5.6501e-02, -6.1698e-02,  1.3432e-02,\n",
       "         -4.5176e-02,  2.6276e-02, -8.3828e-02,  5.3311e-03,  4.9527e-02,\n",
       "          1.4869e-02, -1.3886e-01, -6.9196e-03, -2.3911e-02, -8.0670e-02,\n",
       "          3.1991e-03,  4.1689e-02, -2.7396e-02,  3.5335e-02, -1.0276e-01,\n",
       "         -2.7374e-02, -4.4386e-02, -6.5570e-02,  1.4191e-02,  5.4543e-02,\n",
       "         -4.7281e-03,  2.1144e-02, -5.5454e-02,  8.9045e-02, -8.9299e-04,\n",
       "          7.2535e-02, -7.8848e-03, -1.0387e-01,  2.2219e-02, -3.6113e-02,\n",
       "          3.3401e-02,  4.8602e-02, -3.2317e-02,  6.9946e-02, -5.2012e-03,\n",
       "         -1.1350e-02, -9.8057e-02,  3.0215e-02,  4.5159e-02, -4.0680e-02,\n",
       "         -3.4073e-02,  1.8713e-02,  7.7414e-02,  4.1103e-02, -1.1647e-02,\n",
       "          9.3526e-02, -6.4698e-02, -5.9920e-02,  2.3478e-02,  2.9114e-02,\n",
       "         -9.3531e-02,  1.7802e-02,  5.5459e-02,  8.9963e-03, -5.7461e-02,\n",
       "         -6.8839e-02, -8.5954e-03, -3.2839e-02, -4.9344e-03,  1.3878e-02,\n",
       "          1.0005e-01, -1.4150e-02]], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Привет мир!'\n",
    "emb = embed_bert_cls(text, model, tokenizer)\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.33037819e-02, -1.67655479e-02,  7.06371572e-03, -6.28643557e-02,\n",
       "        1.53607605e-02,  2.51567196e-02, -2.96193752e-02, -6.41856790e-02,\n",
       "       -8.69478062e-02, -1.57285444e-02,  4.89323698e-02,  7.96187147e-02,\n",
       "        7.10701570e-02,  1.90353021e-02,  2.18212791e-02, -7.71338567e-02,\n",
       "        6.42706007e-02, -1.89793706e-02, -2.63677258e-02,  2.55884472e-02,\n",
       "       -6.40277611e-03,  3.75393890e-02,  5.61624654e-02, -7.68886181e-04,\n",
       "        8.80523548e-02, -9.02578793e-03, -4.43253368e-02,  5.59794856e-03,\n",
       "       -2.90220901e-02,  6.00256063e-02, -3.91920805e-02, -5.95636331e-02,\n",
       "        1.04806997e-01, -2.74759550e-02, -2.09237374e-02,  6.14484623e-02,\n",
       "       -3.79129536e-02, -3.50644179e-02, -1.13193601e-01, -7.25624012e-03,\n",
       "       -1.29467472e-01,  1.32998660e-01,  1.11125618e-01, -1.05387792e-01,\n",
       "        1.84967555e-02,  6.78288415e-02,  1.73654724e-02, -1.59727167e-02,\n",
       "        1.00372976e-03, -6.83398396e-02,  6.00939319e-02, -6.07540980e-02,\n",
       "        1.16324853e-02, -1.87639408e-02, -7.23807886e-02,  7.49514475e-02,\n",
       "        1.10915631e-01, -1.36107551e-02, -2.58808155e-02, -2.40475722e-02,\n",
       "        1.71804447e-02,  1.05718367e-01, -3.52799855e-02,  6.93782717e-02,\n",
       "        2.72380840e-02,  7.18914643e-02,  4.94295321e-02,  3.93455178e-02,\n",
       "       -7.37648457e-02,  5.16029298e-02,  6.70910627e-02, -6.27036616e-02,\n",
       "       -9.94532928e-02, -4.93808165e-02,  3.45772691e-02, -1.68846529e-02,\n",
       "       -2.90745609e-02, -3.00907157e-03, -6.36492446e-02,  5.80196679e-02,\n",
       "        2.87183933e-02,  1.03096157e-01, -1.53482531e-03,  2.47128145e-03,\n",
       "       -1.24707902e-02,  1.84906982e-02, -4.77498723e-03, -1.01937670e-02,\n",
       "        6.94087371e-02,  1.17108924e-02,  1.69479046e-02,  1.99001692e-02,\n",
       "        1.02018788e-01, -2.08643395e-02,  6.70525944e-03,  4.30936702e-02,\n",
       "        6.76257461e-02,  9.48665366e-02, -7.97913298e-02, -5.36067458e-03,\n",
       "       -2.45892406e-02, -1.58479810e-02, -3.51777636e-02, -7.69303888e-02,\n",
       "       -2.29561217e-02, -6.14226274e-02, -6.06133509e-03, -2.84504844e-03,\n",
       "        1.69525146e-02, -4.33840528e-02,  7.04689398e-02,  3.97105291e-02,\n",
       "        9.03095976e-02,  1.14425391e-01,  4.71880585e-02,  8.31157863e-02,\n",
       "       -8.12531412e-02, -7.53656542e-03,  1.45039782e-02, -5.19495532e-02,\n",
       "        2.93878410e-02, -4.17007841e-02, -5.30273989e-02,  5.75515330e-02,\n",
       "       -2.16642357e-02, -6.43739011e-03,  2.99985036e-02,  7.59837329e-02,\n",
       "        4.63942029e-02,  4.46742326e-02, -9.40182656e-02,  2.10457891e-02,\n",
       "       -7.40568899e-03,  3.33235934e-02,  1.98901445e-02,  3.08210086e-02,\n",
       "       -2.12678462e-02,  5.18429689e-02,  3.83601636e-02, -6.24488341e-03,\n",
       "        2.36984808e-02,  7.05676526e-02,  6.83088899e-02, -1.54256122e-02,\n",
       "        3.20480727e-02, -6.87091872e-02,  3.69378217e-02,  5.74391522e-03,\n",
       "        4.62789573e-02,  3.83873619e-02,  3.26712839e-02,  5.04141450e-02,\n",
       "        7.06980471e-03, -4.97262068e-02, -1.83812417e-02,  3.99111165e-03,\n",
       "        2.57330649e-02,  2.03679018e-02, -6.34888113e-02, -3.41239572e-02,\n",
       "       -2.43360316e-03,  6.74777403e-02, -9.05379746e-03, -5.88290356e-02,\n",
       "       -2.59724017e-02, -5.03240265e-02, -2.38745869e-03, -1.98704517e-03,\n",
       "       -1.95712280e-02,  5.82334772e-03,  1.60142615e-01,  1.14678673e-01,\n",
       "        1.83556403e-03,  7.87957013e-02, -4.99515906e-02, -9.73445736e-03,\n",
       "        1.04370201e-02, -1.57687850e-02, -2.94537041e-02,  8.20804015e-02,\n",
       "        4.48757075e-02, -3.13021503e-02,  4.80193868e-02,  1.14160068e-02,\n",
       "        1.10872211e-02,  3.50164063e-02,  2.65085064e-02,  3.80779430e-02,\n",
       "       -7.70580117e-03,  3.05277836e-02, -2.25135256e-02, -1.10132601e-02,\n",
       "       -2.07886156e-02,  1.34909283e-02,  4.56029661e-02, -3.81949753e-01,\n",
       "        4.06268835e-02,  1.04613967e-01,  3.14169633e-03,  1.77781861e-02,\n",
       "        9.81659070e-02, -7.76718333e-02, -3.25718373e-02, -3.38851251e-02,\n",
       "        1.01981498e-01, -2.74499003e-02,  7.12979510e-02, -9.79531277e-03,\n",
       "       -1.64567027e-02,  5.79429418e-03, -9.95751098e-02, -4.62819003e-02,\n",
       "       -4.78793308e-02,  2.64916359e-03,  1.89907420e-02, -1.20916128e-01,\n",
       "        4.72991057e-02, -3.47018465e-02, -6.48617325e-03, -5.95681369e-02,\n",
       "       -8.19525495e-03,  5.49282841e-02, -2.23605894e-02,  8.04683715e-02,\n",
       "        6.78008944e-02, -4.57340255e-02, -6.68987108e-04,  6.25265837e-02,\n",
       "       -4.80737016e-02,  7.81764928e-03, -1.07523063e-02, -4.62925397e-02,\n",
       "        2.94204950e-02, -5.90630248e-02,  5.21397702e-02,  3.47027965e-02,\n",
       "        2.14902423e-02, -1.05633788e-01, -2.84684394e-02, -6.55864878e-03,\n",
       "       -2.42032781e-02, -8.90062898e-02, -1.77102615e-04, -5.76009080e-02,\n",
       "       -1.56597290e-02,  1.89236011e-02, -6.72159111e-03, -5.65009154e-02,\n",
       "       -6.16976433e-02,  1.34319896e-02, -4.51764911e-02,  2.62764730e-02,\n",
       "       -8.38283300e-02,  5.33109391e-03,  4.95273732e-02,  1.48687484e-02,\n",
       "       -1.38858035e-01, -6.91963453e-03, -2.39111148e-02, -8.06704834e-02,\n",
       "        3.19910841e-03,  4.16885912e-02, -2.73961816e-02,  3.53349820e-02,\n",
       "       -1.02757588e-01, -2.73744166e-02, -4.43856865e-02, -6.55697137e-02,\n",
       "        1.41909989e-02,  5.45433387e-02, -4.72813100e-03,  2.11442336e-02,\n",
       "       -5.54535501e-02,  8.90448838e-02, -8.92993819e-04,  7.25353286e-02,\n",
       "       -7.88481906e-03, -1.03865333e-01,  2.22190917e-02, -3.61131020e-02,\n",
       "        3.34008783e-02,  4.86015789e-02, -3.23172063e-02,  6.99459240e-02,\n",
       "       -5.20124380e-03, -1.13503421e-02, -9.80571806e-02,  3.02147232e-02,\n",
       "        4.51589897e-02, -4.06798311e-02, -3.40725146e-02,  1.87132079e-02,\n",
       "        7.74139538e-02,  4.11030762e-02, -1.16471425e-02,  9.35260057e-02,\n",
       "       -6.46976605e-02, -5.99202886e-02,  2.34777909e-02,  2.91142054e-02,\n",
       "       -9.35310423e-02,  1.78023931e-02,  5.54590933e-02,  8.99633951e-03,\n",
       "       -5.74611127e-02, -6.88386634e-02, -8.59537721e-03, -3.28387618e-02,\n",
       "       -4.93441895e-03,  1.38776256e-02,  1.00054309e-01, -1.41500244e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 312])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 312)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(emb.cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#model = BertModel.from_pretrained(\"/home/sondors/Documents/price/BERT_training/bert-base-uncased\")#.to('cuda')\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"/home/sondors/Documents/price/BERT_training/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/sondors/Documents/price/BERT_training/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/sondors/anaconda3/envs/torch_gpu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "from sklearn import model_selection\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self, bert_path):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert_path = bert_path\n",
    "        self.bert = transformers.BertModel.from_pretrained(self.bert_path, return_dict=False)\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768, 30)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        bo = self.bert_drop(o2)\n",
    "        return self.out(bo)\n",
    "\n",
    "PATH = \"/home/sondors/Documents/price/BERT_training/ckpt/kaggle/model_5_0.0.pt\"  \n",
    "\n",
    "lr=3e-5\n",
    "model = BERTBaseUncased(\"/home/sondors/Documents/price/BERT_training/bert-base-uncased\")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['spearman']\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/sondors/Documents/price/BERT_training/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = \"123\"\n",
    "torch.save(model, pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 model.save_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">''</span>)                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sondors/anaconda3/envs/torch_gpu/lib/python3.8/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">12</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">69</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getattr__</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1266 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>modules = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__dict__</span>[<span style=\"color: #808000; text-decoration-color: #808000\">'_modules'</span>]                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1267 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> name <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> modules:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1268 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> modules[name]                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1269 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">AttributeError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"'{}' object has no attribute '{}'\"</span>.format(                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1270 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>).<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>, name))                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1271 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1272 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__setattr__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, name: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, value: Union[Tensor, <span style=\"color: #808000; text-decoration-color: #808000\">'Module'</span>]) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERTBaseUncased'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'save_pretrained'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 model.save_pretrained(\u001b[33m'\u001b[0m\u001b[33m'\u001b[0m)                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sondors/anaconda3/envs/torch_gpu/lib/python3.8/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m12\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m69\u001b[0m in \u001b[92m__getattr__\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1266 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodules = \u001b[96mself\u001b[0m.\u001b[91m__dict__\u001b[0m[\u001b[33m'\u001b[0m\u001b[33m_modules\u001b[0m\u001b[33m'\u001b[0m]                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1267 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m name \u001b[95min\u001b[0m modules:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1268 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m modules[name]                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1269 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mAttributeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33m'\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m object has no attribute \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\"\u001b[0m.format(                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1270 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mtype\u001b[0m(\u001b[96mself\u001b[0m).\u001b[91m__name__\u001b[0m, name))                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1271 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1272 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__setattr__\u001b[0m(\u001b[96mself\u001b[0m, name: \u001b[96mstr\u001b[0m, value: Union[Tensor, \u001b[33m'\u001b[0m\u001b[33mModule\u001b[0m\u001b[33m'\u001b[0m]) -> \u001b[94mNone\u001b[0m:             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'BERTBaseUncased'\u001b[0m object has no attribute \u001b[32m'save_pretrained'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.save_pretrained('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTBaseUncased(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (bert_drop): Dropout(p=0.3, inplace=False)\n",
       "  (out): Linear(in_features=768, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model = torch.load(pth)\n",
    "the_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>text = <span style=\"color: #808000; text-decoration-color: #808000\">'Планшетный компьютер 7.85\" Effire CityNight D8 RockChip RK3188 1,8Ггц/1Гб/8Гб/7.</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>encoded_input = tokenizer(text, return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">'pt'</span>)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3 output = model(<span style=\"color: #808000; text-decoration-color: #808000\">'input_ids'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">'token_type_ids'</span>, encoded_input)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>output                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sondors/anaconda3/envs/torch_gpu/lib/python3.8/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">94</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">19</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.out = nn.Linear(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">768</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">30</span>)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, ids, mask, token_type_ids):                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>19 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>_, o2 = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>bo = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bert_drop(o2)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.out(bo)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sondors/anaconda3/envs/torch_gpu/lib/python3.8/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">94</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sondors/anaconda3/envs/torch_gpu/lib/python3.8/site-packages/transformers/models/bert/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">mode</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ling_bert.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">962</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 959 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> input_ids <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> inputs_embeds <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 960 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"You cannot specify both input_ids and inputs_embeds at the</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 961 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> input_ids <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 962 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>input_shape = input_ids.size()                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 963 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> inputs_embeds <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 964 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>input_shape = inputs_embeds.size()[:-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 965 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'size'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m3\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0mtext = \u001b[33m'\u001b[0m\u001b[33mПланшетный компьютер 7.85\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m Effire CityNight D8 RockChip RK3188 1,8Ггц/1Гб/8Гб/7.\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0mencoded_input = tokenizer(text, return_tensors=\u001b[33m'\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m'\u001b[0m)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3 output = model(\u001b[33m'\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mtoken_type_ids\u001b[0m\u001b[33m'\u001b[0m, encoded_input)                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0moutput                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sondors/anaconda3/envs/torch_gpu/lib/python3.8/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m11\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m94\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m19\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.out = nn.Linear(\u001b[94m768\u001b[0m, \u001b[94m30\u001b[0m)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, ids, mask, token_type_ids):                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m19 \u001b[2m│   │   \u001b[0m_, o2 = \u001b[96mself\u001b[0m.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   \u001b[0mbo = \u001b[96mself\u001b[0m.bert_drop(o2)                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.out(bo)                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sondors/anaconda3/envs/torch_gpu/lib/python3.8/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m11\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m94\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sondors/anaconda3/envs/torch_gpu/lib/python3.8/site-packages/transformers/models/bert/\u001b[0m\u001b[1;33mmode\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mling_bert.py\u001b[0m:\u001b[94m962\u001b[0m in \u001b[92mforward\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 959 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m input_ids \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m inputs_embeds \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 960 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mYou cannot specify both input_ids and inputs_embeds at the\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 961 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m input_ids \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 962 \u001b[2m│   │   │   \u001b[0minput_shape = input_ids.size()                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 963 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m inputs_embeds \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 964 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_shape = inputs_embeds.size()[:-\u001b[94m1\u001b[0m]                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 965 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'str'\u001b[0m object has no attribute \u001b[32m'size'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = 'Планшетный компьютер 7.85\" Effire CityNight D8 RockChip RK3188 1,8Ггц/1Гб/8Гб/7.85\" 1024*768/WIFI/Bluetooth/Android 4.1'\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model('input_ids', 'token_type_ids', encoded_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>embeddings = torch.nn.functional.normalize(embeddings)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> embeddings<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#[0].cpu().numpy()</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span>text = <span style=\"color: #808000; text-decoration-color: #808000\">\"Replace me by any text you'd like.\"</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>10 output0 = embed_bert_cls(text, model, tokenizer)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">embed_bert_cls</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">embed_bert_cls</span>(text, model, tokenizer):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>t = tokenizer(text, padding=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, truncation=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">'pt'</span>)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 5 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>model_output = model(**{k: v.to(model.device) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> k, v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> t.items()})               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>embeddings = model_output.last_hidden_state[:, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, :]                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>embeddings = torch.nn.functional.normalize(embeddings)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> embeddings<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#[0].cpu().numpy()</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;dictcomp&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">embed_bert_cls</span>(text, model, tokenizer):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>t = tokenizer(text, padding=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, truncation=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">'pt'</span>)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 5 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>model_output = model(**{k: v.to(model.device) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> k, v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> t.items()})               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>embeddings = model_output.last_hidden_state[:, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, :]                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>embeddings = torch.nn.functional.normalize(embeddings)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> embeddings<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#[0].cpu().numpy()</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/sondors/anaconda3/envs/torch_gpu/lib/python3.8/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">12</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">69</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getattr__</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1266 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>modules = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__dict__</span>[<span style=\"color: #808000; text-decoration-color: #808000\">'_modules'</span>]                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1267 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> name <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> modules:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1268 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> modules[name]                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1269 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">AttributeError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"'{}' object has no attribute '{}'\"</span>.format(                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1270 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>).<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>, name))                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1271 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1272 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__setattr__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, name: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, value: Union[Tensor, <span style=\"color: #808000; text-decoration-color: #808000\">'Module'</span>]) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERTBaseUncased'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'device'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m10\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   \u001b[0membeddings = torch.nn.functional.normalize(embeddings)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m embeddings\u001b[2m#[0].cpu().numpy()\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0mtext = \u001b[33m\"\u001b[0m\u001b[33mReplace me by any text you\u001b[0m\u001b[33m'\u001b[0m\u001b[33md like.\u001b[0m\u001b[33m\"\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m10 output0 = embed_bert_cls(text, model, tokenizer)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92membed_bert_cls\u001b[0m:\u001b[94m5\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92membed_bert_cls\u001b[0m(text, model, tokenizer):                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[2m│   \u001b[0mt = tokenizer(text, padding=\u001b[94mTrue\u001b[0m, truncation=\u001b[94mTrue\u001b[0m, return_tensors=\u001b[33m'\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m'\u001b[0m)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 5 \u001b[2m│   │   \u001b[0mmodel_output = model(**{k: v.to(model.device) \u001b[94mfor\u001b[0m k, v \u001b[95min\u001b[0m t.items()})               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0membeddings = model_output.last_hidden_state[:, \u001b[94m0\u001b[0m, :]                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   \u001b[0membeddings = torch.nn.functional.normalize(embeddings)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m embeddings\u001b[2m#[0].cpu().numpy()\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<dictcomp>\u001b[0m:\u001b[94m5\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92membed_bert_cls\u001b[0m(text, model, tokenizer):                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[2m│   \u001b[0mt = tokenizer(text, padding=\u001b[94mTrue\u001b[0m, truncation=\u001b[94mTrue\u001b[0m, return_tensors=\u001b[33m'\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m'\u001b[0m)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 5 \u001b[2m│   │   \u001b[0mmodel_output = model(**{k: v.to(model.device) \u001b[94mfor\u001b[0m k, v \u001b[95min\u001b[0m t.items()})               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0membeddings = model_output.last_hidden_state[:, \u001b[94m0\u001b[0m, :]                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   \u001b[0membeddings = torch.nn.functional.normalize(embeddings)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m embeddings\u001b[2m#[0].cpu().numpy()\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/sondors/anaconda3/envs/torch_gpu/lib/python3.8/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m12\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m69\u001b[0m in \u001b[92m__getattr__\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1266 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodules = \u001b[96mself\u001b[0m.\u001b[91m__dict__\u001b[0m[\u001b[33m'\u001b[0m\u001b[33m_modules\u001b[0m\u001b[33m'\u001b[0m]                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1267 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m name \u001b[95min\u001b[0m modules:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1268 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m modules[name]                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1269 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mAttributeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33m'\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m object has no attribute \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\"\u001b[0m.format(                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1270 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mtype\u001b[0m(\u001b[96mself\u001b[0m).\u001b[91m__name__\u001b[0m, name))                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1271 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1272 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__setattr__\u001b[0m(\u001b[96mself\u001b[0m, name: \u001b[96mstr\u001b[0m, value: Union[Tensor, \u001b[33m'\u001b[0m\u001b[33mModule\u001b[0m\u001b[33m'\u001b[0m]) -> \u001b[94mNone\u001b[0m:             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'BERTBaseUncased'\u001b[0m object has no attribute \u001b[32m'device'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings#[0].cpu().numpy()\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "output0 = embed_bert_cls(text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(output0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Replace me by any text you'd like.\"\n",
    "text = 'Планшетный компьютер 7.85\" Effire CityNight D8 RockChip RK3188 1,8Ггц/1Гб/8Гб/7.85\" 1024*768/WIFI/Bluetooth/Android 4.1'\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3952, -0.2877,  0.7457,  ..., -0.2125,  0.5516,  0.3017],\n",
       "         [-0.2746, -0.6673,  0.4330,  ..., -0.0762,  1.5857,  0.1402],\n",
       "         [ 0.2217, -0.5225, -0.1275,  ...,  0.2485,  0.8117, -1.3848],\n",
       "         ...,\n",
       "         [-0.8447, -0.5066,  0.8866,  ...,  0.2838,  0.0557, -1.1005],\n",
       "         [-0.7746, -0.7880,  0.0730,  ..., -0.0773,  0.2648, -0.0038],\n",
       "         [ 0.3679,  0.1919, -0.2482,  ...,  0.1927, -0.2607, -0.1649]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.4992, -0.4096, -0.9566,  0.6411,  0.6987, -0.1547,  0.1421,  0.3253,\n",
       "         -0.7286, -0.9997, -0.1223,  0.7160,  0.8189,  0.6701,  0.5414, -0.4162,\n",
       "          0.1531, -0.3532,  0.2367,  0.6040,  0.4951,  1.0000, -0.2881,  0.3627,\n",
       "          0.5153,  0.9100, -0.3149,  0.6223,  0.7554,  0.5477, -0.4052,  0.3207,\n",
       "         -0.9313, -0.3184, -0.9699, -0.8659,  0.4475, -0.2641, -0.1071, -0.1527,\n",
       "         -0.6108,  0.3215,  0.9994,  0.1373,  0.4663, -0.2323, -1.0000,  0.2205,\n",
       "         -0.4444,  0.8705,  0.7977,  0.8796,  0.1858,  0.2512,  0.3276, -0.3812,\n",
       "         -0.1569,  0.0486, -0.4025, -0.4721, -0.5195,  0.5242, -0.8198, -0.4657,\n",
       "          0.8133,  0.9079, -0.2132, -0.3694, -0.0530,  0.0741,  0.2008,  0.2336,\n",
       "         -0.4162, -0.7372,  0.6327,  0.4029, -0.6471,  1.0000, -0.2887, -0.8186,\n",
       "          0.8824,  0.7862,  0.5915, -0.7046,  0.7845, -1.0000,  0.4631, -0.0785,\n",
       "         -0.8959,  0.3935,  0.5445,  0.0039,  0.8453,  0.6239, -0.5541, -0.6137,\n",
       "         -0.3396, -0.8923, -0.4387, -0.3876,  0.2743, -0.2977, -0.3548, -0.2688,\n",
       "          0.4049, -0.4179, -0.0250,  0.4540,  0.1832,  0.4740,  0.6192, -0.5283,\n",
       "          0.3460, -0.7524,  0.3396, -0.2962, -0.9101, -0.6517, -0.9108,  0.4725,\n",
       "         -0.2213, -0.2572,  0.5421, -0.5647,  0.3500, -0.2170, -0.8133, -1.0000,\n",
       "         -0.5095, -0.6983, -0.1512, -0.3457, -0.8195, -0.8381,  0.5990,  0.7711,\n",
       "          0.2432,  0.9986, -0.3059,  0.7367, -0.2459, -0.5266,  0.5329, -0.4508,\n",
       "          0.8266, -0.4218, -0.1507,  0.1652, -0.5658,  0.1963, -0.7756, -0.2710,\n",
       "         -0.7683, -0.5157, -0.4109,  0.6476, -0.6627, -0.9229, -0.3510, -0.0921,\n",
       "         -0.4380,  0.5471,  0.6653,  0.2523, -0.2813,  0.4792,  0.1558,  0.4444,\n",
       "         -0.5685, -0.1217,  0.2962, -0.3596, -0.9579, -0.8554, -0.1976,  0.1578,\n",
       "          0.9034,  0.3656,  0.3828,  0.4896, -0.3630,  0.3857, -0.8733,  0.8760,\n",
       "         -0.2392,  0.3541, -0.8067,  0.7845, -0.3829,  0.3960,  0.4059, -0.5939,\n",
       "         -0.3767, -0.2309, -0.4974, -0.2708, -0.8775,  0.0378, -0.2251, -0.3967,\n",
       "         -0.2316,  0.6455,  0.7751,  0.1417,  0.5334,  0.5496, -0.4255, -0.2792,\n",
       "          0.0048,  0.1510,  0.3188,  0.9013, -0.8546, -0.0846, -0.6032, -0.9115,\n",
       "         -0.0553, -0.5331, -0.0373, -0.6682,  0.6546, -0.6431,  0.3419,  0.1826,\n",
       "         -0.4853, -0.3948,  0.0872, -0.5673,  0.5226, -0.2976,  0.8867,  0.9551,\n",
       "         -0.4663, -0.3649,  0.9444, -0.9274, -0.6206,  0.2600, -0.2006,  0.4981,\n",
       "         -0.5613,  0.8834,  0.9287,  0.4298, -0.3894, -0.8979,  0.2018, -0.5360,\n",
       "          0.0197,  0.4647,  0.8823,  0.6475,  0.3827, -0.1937, -0.3201,  0.5091,\n",
       "         -0.7961, -0.8159, -0.9199, -0.1073, -0.9236,  0.8005,  0.2788,  0.7944,\n",
       "         -0.3863, -0.6386, -0.6277,  0.4418,  0.2440,  0.8614, -0.4297, -0.3304,\n",
       "         -0.4924, -0.7500,  0.0910, -0.1582, -0.5427, -0.0727, -0.5872,  0.4004,\n",
       "          0.3300,  0.4190, -0.8844,  0.8865,  1.0000,  0.7473,  0.5166,  0.2176,\n",
       "         -0.9999, -0.7275,  0.9999, -0.9693, -1.0000, -0.3890, -0.5398,  0.3447,\n",
       "         -1.0000, -0.3365, -0.0648, -0.4347,  0.7753,  0.7966,  0.2827, -1.0000,\n",
       "         -0.0284,  0.6415, -0.7056,  0.8385, -0.6250,  0.7277,  0.3277,  0.4180,\n",
       "         -0.1354,  0.4449, -0.9500, -0.6073, -0.6208, -0.7668,  0.9994,  0.0733,\n",
       "         -0.7720, -0.4463,  0.5373, -0.0952,  0.1068, -0.7433, -0.3908,  0.2697,\n",
       "          0.6241,  0.1894,  0.3588,  0.0255,  0.2668,  0.7020, -0.0759,  0.5620,\n",
       "         -0.7017, -0.1861, -0.4324, -0.1113, -0.7566, -0.9093,  0.7553, -0.4378,\n",
       "          0.7533,  1.0000,  0.7431, -0.4090,  0.5924,  0.3229, -0.5991,  1.0000,\n",
       "          0.7247, -0.8499, -0.6823,  0.7082, -0.4568, -0.6097,  0.9898, -0.1861,\n",
       "         -0.7266, -0.1553,  0.9347, -0.9153,  0.9989, -0.4890, -0.7396,  0.6559,\n",
       "          0.5915, -0.4443, -0.3103,  0.1430, -0.7994,  0.3677, -0.1635,  0.4641,\n",
       "          0.3393, -0.0625,  0.6467,  0.0296, -0.6888,  0.1983, -0.7452, -0.4678,\n",
       "          0.9678,  0.4749, -0.1282, -0.0266, -0.2988, -0.6530, -0.7138,  0.6285,\n",
       "          1.0000, -0.1757,  0.9173, -0.1891, -0.1447,  0.1483,  0.5381,  0.5149,\n",
       "         -0.2349, -0.4325,  0.6644, -0.2489, -0.9505,  0.4675,  0.3684, -0.3071,\n",
       "          0.9997,  0.4189,  0.3669,  0.3459,  0.9360,  0.1098,  0.0830,  0.8703,\n",
       "          0.8292, -0.3256,  0.7053,  0.0338, -0.8150, -0.5022, -0.4683,  0.1439,\n",
       "         -0.8464, -0.1332, -0.6003,  0.7525,  0.9534,  0.4370,  0.2617,  0.8464,\n",
       "          1.0000, -0.9490,  0.2970,  0.8321, -0.0090, -0.9999, -0.2901, -0.3113,\n",
       "         -0.1636, -0.8672, -0.3612,  0.2632, -0.7830,  0.7896,  0.5487, -0.6382,\n",
       "         -0.8079, -0.4959,  0.2239,  0.0748, -0.9906, -0.4510, -0.1719, -0.0126,\n",
       "         -0.2248, -0.4034, -0.1836, -0.3377,  0.4550, -0.1247,  0.6549,  0.9166,\n",
       "          0.5214, -0.9406, -0.4699, -0.1748, -0.5552,  0.5831, -0.6355, -0.9180,\n",
       "         -0.3309,  1.0000, -0.0633,  0.8713,  0.5392,  0.2455, -0.2851,  0.2986,\n",
       "          0.9666,  0.3548, -0.7240, -0.8355,  0.8124, -0.5061,  0.5099,  0.6421,\n",
       "          0.8674,  0.2326,  0.6289,  0.0058, -0.0227,  0.1162,  0.7400, -0.2722,\n",
       "         -0.1434, -0.1543, -0.2399, -0.2437,  0.4732,  1.0000,  0.3570,  0.7061,\n",
       "         -0.9276, -0.8317, -0.4993,  1.0000,  0.6681,  0.1583,  0.5170,  0.6779,\n",
       "         -0.1910,  0.0791, -0.1542, -0.3697,  0.3033,  0.2946,  0.6588, -0.3803,\n",
       "         -0.8106, -0.5963,  0.3240, -0.6490,  1.0000, -0.4578, -0.1666, -0.1500,\n",
       "         -0.4994, -0.9058, -0.0058, -0.7934, -0.2700,  0.2124,  0.6453,  0.3155,\n",
       "         -0.6629, -0.6993,  0.8832,  0.6920, -0.8386, -0.7189,  0.7054, -0.8451,\n",
       "          0.4355,  1.0000,  0.4273,  0.4835,  0.0428, -0.1982,  0.2528, -0.4801,\n",
       "          0.4694, -0.5675, -0.2777, -0.2800,  0.1855, -0.2004, -0.8647,  0.3944,\n",
       "          0.3355, -0.5978, -0.6303, -0.0213,  0.3121,  0.6195, -0.3474, -0.2334,\n",
       "          0.2167,  0.0428, -0.4794, -0.2903, -0.3626, -1.0000,  0.4089, -1.0000,\n",
       "          0.7109,  0.4792, -0.1531,  0.4183,  0.3013,  0.6705, -0.1652, -0.8511,\n",
       "          0.0183,  0.4061, -0.2430, -0.2026, -0.1575,  0.3011, -0.0650,  0.2856,\n",
       "         -0.8244,  0.6470, -0.2666,  1.0000,  0.1526, -0.5565, -0.4599,  0.3299,\n",
       "         -0.2705,  1.0000,  0.0848, -0.8037,  0.4481, -0.7921, -0.6276,  0.4379,\n",
       "         -0.0329, -0.7329, -0.8740,  0.5187,  0.0668, -0.6374,  0.4887, -0.4119,\n",
       "         -0.3868,  0.2485,  0.9047,  0.8926,  0.6685,  0.3180, -0.5120, -0.4705,\n",
       "          0.7580,  0.3824, -0.3631,  0.2012,  1.0000,  0.3887, -0.7273,  0.1501,\n",
       "         -0.6699, -0.3804, -0.7372,  0.2724,  0.4185,  0.6878, -0.1519,  0.7457,\n",
       "         -0.7683,  0.0592, -0.4112, -0.7568,  0.2556, -0.5382, -0.8219, -0.8651,\n",
       "          0.6038, -0.2914, -0.1757,  0.1798,  0.1717,  0.3737,  0.3215, -1.0000,\n",
       "          0.7977,  0.3361,  0.8689,  0.7308,  0.7739,  0.5599,  0.3719, -0.8264,\n",
       "         -0.2284, -0.1975, -0.2713,  0.6474,  0.5497,  0.5243,  0.1960, -0.4384,\n",
       "         -0.4494, -0.6063, -0.6606, -0.9326,  0.5182, -0.5704, -0.2143,  0.8365,\n",
       "          0.2494, -0.1993, -0.4728, -0.8664,  0.4710,  0.4849,  0.1190,  0.0391,\n",
       "          0.3086,  0.2736,  0.5474,  0.8879, -0.8879,  0.2254, -0.7912,  0.3826,\n",
       "          0.9068, -0.8143,  0.2303,  0.4045, -0.3407,  0.2779, -0.4302, -0.5376,\n",
       "          0.8385, -0.3659,  0.4920, -0.3638,  0.0242, -0.4222, -0.1984, -0.2028,\n",
       "         -0.5783,  0.5288, -0.0948,  0.5654,  0.9027, -0.0672, -0.5746, -0.1525,\n",
       "         -0.7842, -0.6684,  0.1312, -0.1191, -0.4894,  0.8799, -0.0111,  0.9180,\n",
       "          0.2671, -0.2925, -0.1313, -0.4553,  0.2434, -0.4488, -0.6173, -0.5872,\n",
       "          0.5115,  0.2738,  1.0000, -0.7583, -0.8059, -0.5012, -0.4103,  0.4764,\n",
       "         -0.5699, -1.0000,  0.3224, -0.5673,  0.7563, -0.3873,  0.8721, -0.4928,\n",
       "         -0.7704, -0.4158,  0.6565,  0.7109, -0.3337, -0.4701,  0.5320, -0.5913,\n",
       "          0.9596,  0.3864, -0.3454, -0.0831,  0.6471, -0.8880, -0.5946,  0.3568]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 74])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 74, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(output[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "text2 = \"Replace me\"\n",
    "encoded_input2 = tokenizer(text2, return_tensors='pt')\n",
    "output2 = model(**encoded_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7368039]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(output[1].detach().numpy(), output2[1].detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
